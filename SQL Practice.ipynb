{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70317934-456a-492f-9697-2bc61ceac671",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Table: Views\n",
    "\n",
    "# +---------------+---------+\n",
    "# | Column Name   | Type    |\n",
    "# +---------------+---------+\n",
    "# | article_id    | int     |\n",
    "# | author_id     | int     |\n",
    "# | viewer_id     | int     |\n",
    "# | view_date     | date    |\n",
    "# +---------------+---------+\n",
    "# There is no primary key (column with unique values) for this table, the table may have duplicate rows.\n",
    "# Each row of this table indicates that some viewer viewed an article (written by some author) on some date. \n",
    "# Note that equal author_id and viewer_id indicate the same person.\n",
    " \n",
    "# Write a solution to find all the authors that viewed at least one of their own articles.\n",
    "# Return the result table sorted by id in ascending order.\n",
    "# The result format is in the following example.\n",
    "\n",
    "# Example 1:\n",
    "\n",
    "# Input: \n",
    "# Views table:\n",
    "# +------------+-----------+-----------+------------+\n",
    "# | article_id | author_id | viewer_id | view_date  |\n",
    "# +------------+-----------+-----------+------------+\n",
    "# | 1          | 3         | 5         | 2019-08-01 |\n",
    "# | 1          | 3         | 6         | 2019-08-02 |\n",
    "# | 2          | 7         | 7         | 2019-08-01 |\n",
    "# | 2          | 7         | 6         | 2019-08-02 |\n",
    "# | 4          | 7         | 1         | 2019-07-22 |\n",
    "# | 3          | 4         | 4         | 2019-07-21 |\n",
    "# | 3          | 4         | 4         | 2019-07-21 |\n",
    "# +------------+-----------+-----------+------------+\n",
    "\n",
    "# Output: \n",
    "# +------+\n",
    "# | id   |\n",
    "# +------+\n",
    "# | 4    |\n",
    "# | 7    |\n",
    "# +------+\n",
    "\n",
    "from pyspark.sql.functions import col \n",
    "\n",
    "data = [\n",
    "(1, 3, 5, '2019-08-01'),\n",
    "(1, 3, 6, '2019-08-02'),\n",
    "(2, 7, 7, '2019-08-01'),\n",
    "(2, 7, 6, '2019-08-02'),\n",
    "(4, 7, 1, '2019-07-22'),\n",
    "(3, 4, 4, '2019-07-21'),\n",
    "(3, 4, 4, '2019-07-21'),\n",
    "]\n",
    "\n",
    "columns = ['article_id' , 'author_id' , 'viewer_id' , 'view_date']\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "display(df)\n",
    "\n",
    "df1 = df.where(\"author_id = viewer_id\").select(col('author_id').alias('id')).distinct().sort(\"id\", ascending=True)\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56dc7864-b4a4-4a07-8114-e5ab5f7af5d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Table: Tweets\n",
    "\n",
    "# +----------------+---------+\n",
    "# | Column Name    | Type    |\n",
    "# +----------------+---------+\n",
    "# | tweet_id       | int     |\n",
    "# | content        | varchar |\n",
    "# +----------------+---------+\n",
    "# tweet_id is the primary key (column with unique values) for this table.\n",
    "# content consists of alphanumeric characters, '!', or ' ' and no other special characters.\n",
    "# This table contains all the tweets in a social media app.\n",
    "\n",
    "# Write a solution to find the IDs of the invalid tweets. The tweet is invalid if the number of characters used in the content of the tweet is strictly greater than 15.\n",
    "\n",
    "# Return the result table in any order.\n",
    "# The result format is in the following example.\n",
    "\n",
    "# Example 1:\n",
    "\n",
    "# Input: \n",
    "# Tweets table:\n",
    "# +----------+-----------------------------------+\n",
    "# | tweet_id | content                           |\n",
    "# +----------+-----------------------------------+\n",
    "# | 1        | Let us Code                       |\n",
    "# | 2        | More than fifteen chars are here! |\n",
    "# +----------+-----------------------------------+\n",
    "\n",
    "# Output: \n",
    "# +----------+\n",
    "# | tweet_id |\n",
    "# +----------+\n",
    "# | 2        |\n",
    "# +----------+\n",
    "# Explanation: \n",
    "# Tweet 1 has length = 11. It is a valid tweet.\n",
    "# Tweet 2 has length = 33. It is an invalid tweet.\n",
    "\n",
    "data = [\n",
    "(1, 'Let us Code'),\n",
    "(2, 'More than fifteen chars are here!'),\n",
    "]\n",
    "\n",
    "columns = ['tweet_id', 'content']\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "display(df)\n",
    "df1 = df.where(\"len(content)>15\").select(\"tweet_id\")\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1de4dc7c-951b-4e67-857d-39e258540eba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Table: Employees\n",
    "# +---------------+---------+\n",
    "# | Column Name   | Type    |\n",
    "# +---------------+---------+\n",
    "# | id            | int     |\n",
    "# | name          | varchar |\n",
    "# +---------------+---------+\n",
    "# id is the primary key (column with unique values) for this table.\n",
    "# Each row of this table contains the id and the name of an employee in a company.\n",
    " \n",
    "# Table: EmployeeUNI\n",
    "# +---------------+---------+\n",
    "# | Column Name   | Type    |\n",
    "# +---------------+---------+\n",
    "# | id            | int     |\n",
    "# | unique_id     | int     |\n",
    "# +---------------+---------+\n",
    "# (id, unique_id) is the primary key (combination of columns with unique values) for this table.\n",
    "# Each row of this table contains the id and the corresponding unique id of an employee in the company.\n",
    " \n",
    "# Write a solution to show the unique ID of each user, If a user does not have a unique ID replace just show null.\n",
    "# Return the result table in any order.\n",
    "# The result format is in the following example.\n",
    "\n",
    "# Example 1:\n",
    "\n",
    "# Input: \n",
    "# Employees table:\n",
    "# +----+----------+\n",
    "# | id | name     |\n",
    "# +----+----------+\n",
    "# | 1  | Alice    |\n",
    "# | 7  | Bob      |\n",
    "# | 11 | Meir     |\n",
    "# | 90 | Winston  |\n",
    "# | 3  | Jonathan |\n",
    "# +----+----------+\n",
    "# EmployeeUNI table:\n",
    "# +----+-----------+\n",
    "# | id | unique_id |\n",
    "# +----+-----------+\n",
    "# | 3  | 1         |\n",
    "# | 11 | 2         |\n",
    "# | 90 | 3         |\n",
    "# +----+-----------+\n",
    "# Output: \n",
    "# +-----------+----------+\n",
    "# | unique_id | name     |\n",
    "# +-----------+----------+\n",
    "# | null      | Alice    |\n",
    "# | null      | Bob      |\n",
    "# | 2         | Meir     |\n",
    "# | 3         | Winston  |\n",
    "# | 1         | Jonathan |\n",
    "# +-----------+----------+\n",
    "# Explanation: \n",
    "# Alice and Bob do not have a unique ID, We will show null instead.\n",
    "# The unique ID of Meir is 2.\n",
    "# The unique ID of Winston is 3.\n",
    "# The unique ID of Jonathan is 1.\n",
    "\n",
    "col1 = ['id', 'name']\n",
    "data1 = [\n",
    "(1 , 'Alice'),\n",
    "(7 , 'Bob'),\n",
    "(11, 'Meir'),\n",
    "(90, 'Winston'),\n",
    "(3 , 'Jonathan')\n",
    "]\n",
    "\n",
    "col2 = ['id', 'unique_id']\n",
    "data2 = [\n",
    "(3 , 1),\n",
    "(11, 2),\n",
    "(90, 3)\n",
    "]\n",
    "\n",
    "df1 = spark.createDataFrame(data1, col1)\n",
    "df2 = spark.createDataFrame(data2, col2)\n",
    "\n",
    "df = df1.join(df2, df1.id == df2.id, 'left').select('unique_id', 'name')\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3db28f0-f793-4d2e-9060-4c4520df7ed3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1068. Product Sales Analysis I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccc07aac-0084-4bc3-ba13-5747a8bc670a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Table: Sales\n",
    "\n",
    "# +-------------+-------+\n",
    "# | Column Name | Type  |\n",
    "# +-------------+-------+\n",
    "# | sale_id     | int   |\n",
    "# | product_id  | int   |\n",
    "# | year        | int   |\n",
    "# | quantity    | int   |\n",
    "# | price       | int   |\n",
    "# +-------------+-------+\n",
    "# (sale_id, year) is the primary key (combination of columns with unique values) of this table.\n",
    "# product_id is a foreign key (reference column) to Product table.\n",
    "# Each row of this table shows a sale on the product product_id in a certain year.\n",
    "# Note that the price is per unit.\n",
    " \n",
    "\n",
    "# Table: Product\n",
    "\n",
    "# +--------------+---------+\n",
    "# | Column Name  | Type    |\n",
    "# +--------------+---------+\n",
    "# | product_id   | int     |\n",
    "# | product_name | varchar |\n",
    "# +--------------+---------+\n",
    "# product_id is the primary key (column with unique values) of this table.\n",
    "# Each row of this table indicates the product name of each product.\n",
    " \n",
    "\n",
    "# Write a solution to report the product_name, year, and price for each sale_id in the Sales table.\n",
    "\n",
    "# Return the resulting table in any order.\n",
    "\n",
    "# The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "# Example 1:\n",
    "\n",
    "# Input: \n",
    "# Sales table:\n",
    "# +---------+------------+------+----------+-------+\n",
    "# | sale_id | product_id | year | quantity | price |\n",
    "# +---------+------------+------+----------+-------+ \n",
    "# | 1       | 100        | 2008 | 10       | 5000  |\n",
    "# | 2       | 100        | 2009 | 12       | 5000  |\n",
    "# | 7       | 200        | 2011 | 15       | 9000  |\n",
    "# +---------+------------+------+----------+-------+\n",
    "# Product table:\n",
    "# +------------+--------------+\n",
    "# | product_id | product_name |\n",
    "# +------------+--------------+\n",
    "# | 100        | Nokia        |\n",
    "# | 200        | Apple        |\n",
    "# | 300        | Samsung      |\n",
    "# +------------+--------------+\n",
    "# Output: \n",
    "# +--------------+-------+-------+\n",
    "# | product_name | year  | price |\n",
    "# +--------------+-------+-------+\n",
    "# | Nokia        | 2008  | 5000  |\n",
    "# | Nokia        | 2009  | 5000  |\n",
    "# | Apple        | 2011  | 9000  |\n",
    "# +--------------+-------+-------+\n",
    "# Explanation: \n",
    "# From sale_id = 1, we can conclude that Nokia was sold for 5000 in the year 2008.\n",
    "# From sale_id = 2, we can conclude that Nokia was sold for 5000 in the year 2009.\n",
    "# From sale_id = 7, we can conclude that Apple was sold for 9000 in the year 2011.\n",
    "\n",
    "\n",
    "data1 = [\n",
    "(1, 100, 2008, 10, 5000),\n",
    "(2, 100, 2009, 12, 5000),\n",
    "(7, 200, 2011, 15, 9000)\n",
    "]\n",
    "\n",
    "data2 = [\n",
    "(100, 'Nokia'),\n",
    "(200, 'Apple'),\n",
    "(300, 'Samsung')    \n",
    "]\n",
    "\n",
    "col1 = ['sale_id', 'product_id', 'year', 'quantity', 'price']\n",
    "col2 = ['product_id', 'product_name']\n",
    "\n",
    "df1 = spark.createDataFrame(data1, col1)\n",
    "df2 = spark.createDataFrame(data2, col2)\n",
    "\n",
    "df = df1.join(df2,df1.product_id == df2.product_id,'inner').select(\"product_name\", \"year\", \"price\")\n",
    "display(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df1ec8e9-3acc-41b3-af02-c76af79b7a0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1581. Customer Who Visited but Did Not Make\n",
    "## Any Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ed784a8-07bc-449a-ace9-109a1876bf96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Table: Visits\n",
    "\n",
    "# +-------------+---------+\n",
    "# | Column Name | Type    |\n",
    "# +-------------+---------+\n",
    "# | visit_id    | int     |\n",
    "# | customer_id | int     |\n",
    "# +-------------+---------+\n",
    "# visit_id is the column with unique values for this table.\n",
    "# This table contains information about the customers who visited the mall.\n",
    " \n",
    "# Table: Transactions\n",
    "\n",
    "# +----------------+---------+\n",
    "# | Column Name    | Type    |\n",
    "# +----------------+---------+\n",
    "# | transaction_id | int     |\n",
    "# | visit_id       | int     |\n",
    "# | amount         | int     |\n",
    "# +----------------+---------+\n",
    "# transaction_id is column with unique values for this table.\n",
    "# This table contains information about the transactions made during the visit_id.\n",
    " \n",
    "# Write a solution to find the IDs of the users who visited without making any transactions and the number of times they made these types of visits.\n",
    "# Return the result table sorted in any order.\n",
    "# The result format is in the following example.\n",
    "\n",
    " # Example 1:\n",
    "\n",
    "# Input: \n",
    "# Visits\n",
    "# +----------+-------------+\n",
    "# | visit_id | customer_id |\n",
    "# +----------+-------------+\n",
    "# | 1        | 23          |\n",
    "# | 2        | 9           |\n",
    "# | 4        | 30          |\n",
    "# | 5        | 54          |\n",
    "# | 6        | 96          |\n",
    "# | 7        | 54          |\n",
    "# | 8        | 54          |\n",
    "# +----------+-------------+\n",
    "# Transactions\n",
    "# +----------------+----------+--------+\n",
    "# | transaction_id | visit_id | amount |\n",
    "# +----------------+----------+--------+\n",
    "# | 2              | 5        | 310    |\n",
    "# | 3              | 5        | 300    |\n",
    "# | 9              | 5        | 200    |\n",
    "# | 12             | 1        | 910    |\n",
    "# | 13             | 2        | 970    |\n",
    "# +----------------+----------+--------+\n",
    "# Output: \n",
    "# +-------------+----------------+\n",
    "# | customer_id | count_no_trans |\n",
    "# +-------------+----------------+\n",
    "# | 54          | 2              |\n",
    "# | 30          | 1              |\n",
    "# | 96          | 1              |\n",
    "# +-------------+----------------+\n",
    "# Explanation: \n",
    "# Customer with id = 23 visited the mall once and made one transaction during the visit with id = 12.\n",
    "# Customer with id = 9 visited the mall once and made one transaction during the visit with id = 13.\n",
    "# Customer with id = 30 visited the mall once and did not make any transactions.\n",
    "# Customer with id = 54 visited the mall three times. During 2 visits they did not make any transactions, and during one visit they made 3 transactions.\n",
    "# Customer with id = 96 visited the mall once and did not make any transactions.\n",
    "# As we can see, users with IDs 30 and 96 visited the mall one time without making any transactions. Also, user 54 visited the mall twice and did not make any transactions.\n",
    "\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "data1 = [\n",
    "(1, 23),\n",
    "(2, 9 ),\n",
    "(4, 30),\n",
    "(5, 54),\n",
    "(6, 96),\n",
    "(7, 54),\n",
    "(8, 54)\n",
    "]\n",
    "\n",
    "data2 = [\n",
    "(2 , 5, 310),\n",
    "(3 , 5, 300),\n",
    "(9 , 5, 200),\n",
    "(12, 1, 910),\n",
    "(13, 2, 970)\n",
    "]\n",
    "\n",
    "col1 = ['visit_id', 'customer_id']\n",
    "col2 = ['transaction_id', 'visit_id', 'amount']\n",
    "\n",
    "df1 = spark.createDataFrame(data1, col1)\n",
    "df2 = spark.createDataFrame(data2, col2)\n",
    "\n",
    "df = df1.join(df2, df1.visit_id == df2.visit_id, 'left').filter(df2[\"visit_id\"].isNull()).select(df1.visit_id, df1.customer_id).groupBy(\"customer_id\").agg(count(\"visit_id\").alias(\"count_no_trans\"))\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8a59ad4-8b45-4836-9830-5d6e217ebd03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Histogram of Tweets\n",
    "Twitter SQL Interview Question <br>\n",
    "Difficulty: Easy<br>\n",
    "Source: https://datalemur.com/questions/sql-histogram-tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad08ea18-2b7b-4e7d-b33d-d08fae055f5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Assume you're given a table Twitter tweet data, write a query to obtain a histogram of tweets posted per user in 2022. Output the tweet count per user as the bucket and the number of Twitter users who fall into that bucket.\n",
    "\n",
    "# In other words, group the users by the number of tweets they posted in 2022 and count the number of users in each group.\n",
    "\n",
    "# tweets Table:\n",
    "# Column Name\tType\n",
    "# tweet_id\tinteger\n",
    "# user_id\tinteger\n",
    "# msg\tstring\n",
    "# tweet_date\ttimestamp\n",
    "# tweets Example Input:\n",
    "# tweet_id\tuser_id\tmsg\ttweet_date\n",
    "# 214252\t111\tAm considering taking Tesla private at $420. Funding secured.\t12/30/2021 00:00:00\n",
    "# 739252\t111\tDespite the constant negative press covfefe\t01/01/2022 00:00:00\n",
    "# 846402\t111\tFollowing @NickSinghTech on Twitter changed my life!\t02/14/2022 00:00:00\n",
    "# 241425\t254\tIf the salary is so competitive why won’t you tell me what it is?\t03/01/2022 00:00:00\n",
    "# 231574\t148\tI no longer have a manager. I can't be managed\t03/23/2022 00:00:00\n",
    "# Example Output:\n",
    "# tweet_bucket\tusers_num\n",
    "# 1\t2\n",
    "# 2\t1\n",
    "# Explanation:\n",
    "# Based on the example output, there are two users who posted only one tweet in 2022, and one user who posted two tweets in 2022. The query groups the users by the number of tweets they posted and displays the number of users in each group.\n",
    "\n",
    "# The dataset you are querying against may have different input & output - this is just an example!\n",
    "\n",
    "# Solution\n",
    "\n",
    "%sql\n",
    "with tweet_counts AS (\n",
    "SELECT user_id, count(tweet_id) as tweet_counts FROM tweets\n",
    "where Year(tweet_date) = 2022\n",
    "group by user_id)\n",
    "\n",
    "Select tweet_counts as tweet_bucket, count(user_id) as users_num\n",
    "from tweet_counts\n",
    "group by tweet_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "381a46f8-4a3e-4069-809a-6a22555d371e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Salaries Differences\n",
    "Difficulty: Easy<br>\n",
    "Companies: LinkedIn, Dropbox<br>\n",
    "Source: https://platform.stratascratch.com/coding/10308-salaries-differences?code_type=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "622fea98-2cd0-4f09-9346-80488ecb269c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculates the difference between the highest salaries in the marketing and engineering departments. Output just the absolute difference in salaries.\n",
    "\n",
    "# Tables\n",
    "# db_employee\n",
    "# db_dept\n",
    "# More about this question\n",
    "# Hints\n",
    "# Expected Output\n",
    "# db_employee\n",
    "# Preview\n",
    "# department_id:\n",
    "# bigint\n",
    "# first_name:\n",
    "# varchar\n",
    "# id:\n",
    "# bigint\n",
    "# last_name:\n",
    "# varchar\n",
    "# salary:\n",
    "# bigint\n",
    "# db_dept\n",
    "# Preview\n",
    "# department:\n",
    "# varchar\n",
    "# id:\n",
    "# bigint\n",
    "# Recommended Easy Interview Questions\n",
    "# ID 2002\n",
    "# Submission Types\n",
    "# ID 2004\n",
    "# Number of Comments Per User in 30 days before 2020-02-10\n",
    "# ID 2006\n",
    "# Users Activity Per Month Day\n",
    "# Recommended Questions from the Same Companies\n",
    "# ID 9603\n",
    "# Find fare differences on the Titanic using a self join\n",
    "# ID 9921\n",
    "# Department Salaries\n",
    "# ID 9920\n",
    "# Sales Dept Salaries\n",
    "\n",
    "\n",
    "with department_wise_salary as (\n",
    "select d.department, max(e.salary) as highest_salary \n",
    "from db_employee e\n",
    "join db_dept d\n",
    "on e.department_id = d.id\n",
    "where d.department in ('marketing', 'engineering')\n",
    "group by d.department\n",
    ")\n",
    "Select abs(engineering - marketing) from department_wise_salary\n",
    "PIVOT \n",
    "(MAX(highest_salary) \n",
    "for department in ([engineering], [marketing])\n",
    ") as pv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7135c5f3-437f-49f6-b91d-23395e6810f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##User's Third Transaction\n",
    "Uber SQL Interview Question <br>\n",
    "Difficulty: Medium<br>\n",
    "Source: https://datalemur.com/questions/sql-third-transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d672912-bf39-4360-a290-fba08e8fe46a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Assume you are given the table below on Uber transactions made by users. Write a query to obtain the third transaction of every user. Output the user id, spend and transaction date.\n",
    "\n",
    "-- transactions Table:\n",
    "-- Column Name\tType\n",
    "-- user_id\tinteger\n",
    "-- spend\tdecimal\n",
    "-- transaction_date\ttimestamp\n",
    "-- transactions Example Input:\n",
    "-- user_id\tspend\ttransaction_date\n",
    "-- 111\t100.50\t01/08/2022 12:00:00\n",
    "-- 111\t55.00\t01/10/2022 12:00:00\n",
    "-- 121\t36.00\t01/18/2022 12:00:00\n",
    "-- 145\t24.99\t01/26/2022 12:00:00\n",
    "-- 111\t89.60\t02/05/2022 12:00:00\n",
    "-- Example Output:\n",
    "-- user_id\tspend\ttransaction_date\n",
    "-- 111\t89.60\t02/05/2022 12:00:00\n",
    "-- The dataset you are querying against may have different input & output - this is just an example!\n",
    "\n",
    "-- Solution \n",
    "with ranked_transactions AS \n",
    "(\n",
    "SELECT \n",
    "user_id, \n",
    "spend, \n",
    "transaction_date,\n",
    "row_number() over (partition by user_id order by transaction_date) as rnk\n",
    "FROM transactions\n",
    ")\n",
    "\n",
    "Select \n",
    "user_id, \n",
    "spend, \n",
    "transaction_date\n",
    "from ranked_transactions\n",
    "where  rnk = 3;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0562c27-0a96-4e4f-9291-552bf14d9207",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Second Highest Salary\n",
    "FAANG SQL Interview Question<br>\n",
    "Difficulty: Medium<br>\n",
    "Source: https://datalemur.com/questions/sql-second-highest-salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "839e5746-4427-4714-8ed6-3e76a3ddc5e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "-- Imagine you're an HR analyst at a tech company tasked with analyzing employee salaries. Your manager is keen on understanding the pay distribution and asks you to determine the second highest salary among all employees.\n",
    "\n",
    "-- It's possible that multiple employees may share the same second highest salary. In case of duplicate, display the salary only once.\n",
    "\n",
    "-- employee Schema:\n",
    "-- column_name\ttype\tdescription\n",
    "-- employee_id\tinteger\tThe unique ID of the employee.\n",
    "-- name\tstring\tThe name of the employee.\n",
    "-- salary\tinteger\tThe salary of the employee.\n",
    "-- department_id\tinteger\tThe department ID of the employee.\n",
    "-- manager_id\tinteger\tThe manager ID of the employee.\n",
    "-- employee Example Input:\n",
    "-- employee_id\tname\tsalary\tdepartment_id\tmanager_id\n",
    "-- 1\tEmma Thompson\t3800\t1\t6\n",
    "-- 2\tDaniel Rodriguez\t2230\t1\t7\n",
    "-- 3\tOlivia Smith\t2000\t1\t8\n",
    "-- Example Output:\n",
    "-- second_highest_salary\n",
    "-- 2230\n",
    "-- The output represents the second highest salary among all employees. In this case, the second highest salary is $2,230.\n",
    "\n",
    "\n",
    "-- Solution\n",
    "WITH ranked_emp_salary AS\n",
    "(\n",
    "SELECT\n",
    "SALARY,\n",
    "DENSE_RANK() OVER(ORDER BY SALARY DESC) AS salary_rnk\n",
    "FROM employee\n",
    ")\n",
    "SELECT SALARY AS second_highest_salary\n",
    "from ranked_emp_salary\n",
    "where salary_rnk = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08e8371a-7255-43e2-8271-73721a499b60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Active User Retention\n",
    "Facebook SQL Interview Question<br>\n",
    "Difficulty: Hard<br>\n",
    "Source: https://datalemur.com/questions/user-retention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09353278-0f7b-4943-8d61-4d194ac47083",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "-- Assume you're given a table containing information on Facebook user actions. Write a query to obtain number of monthly active users (MAUs) in July 2022, including the month in numerical format \"1, 2, 3\".\n",
    "\n",
    "-- Hint:\n",
    "\n",
    "-- An active user is defined as a user who has performed actions such as 'sign-in', 'like', or 'comment' in both the current month and the previous month.\n",
    "-- user_actions Table:\n",
    "-- Column Name\tType\n",
    "-- user_id\tinteger\n",
    "-- event_id\tinteger\n",
    "-- event_type\tstring (\"sign-in, \"like\", \"comment\")\n",
    "-- event_date\tdatetime\n",
    "-- user_actionsExample Input:\n",
    "-- user_id\tevent_id\tevent_type\tevent_date\n",
    "-- 445\t7765\tsign-in\t05/31/2022 12:00:00\n",
    "-- 742\t6458\tsign-in\t06/03/2022 12:00:00\n",
    "-- 445\t3634\tlike\t06/05/2022 12:00:00\n",
    "-- 742\t1374\tcomment\t06/05/2022 12:00:00\n",
    "-- 648\t3124\tlike\t06/18/2022 12:00:00\n",
    "-- Example Output for June 2022:\n",
    "-- month\tmonthly_active_users\n",
    "-- 6\t1\n",
    "-- Example\n",
    "-- In June 2022, there was only one monthly active user (MAU) with the user_id 445.\n",
    "\n",
    "-- Please note that the output provided is for June 2022 as the user_actions table only contains event dates for that month. You should adapt the solution accordingly for July 2022.\n",
    "\n",
    "-- Solution\n",
    "\n",
    "with monthly_users as (\n",
    "SELECT DISTINCT user_id, \n",
    "date_part('month',event_date) as event_month\n",
    "FROM user_actions\n",
    "where event_date between '2022-06-01' and '2022-07-31'\n",
    "and event_type in ('sign-in', 'like', 'comment')\n",
    "),\n",
    "monthly_active_users as\n",
    "(\n",
    "Select\n",
    "user_id,\n",
    "event_month as month,\n",
    "lag(event_month) over(partition by user_id order by event_month) as prev_month\n",
    "from monthly_users\n",
    ")\n",
    "SELECT\n",
    "month,\n",
    "count(user_id) as monthly_active_users\n",
    "from monthly_active_users\n",
    "where prev_month IS NOT NULL\n",
    "group by month\n",
    "\n",
    "\n",
    "-- Alternate Solution\n",
    "SELECT \n",
    "  EXTRACT(MONTH FROM curr_month.event_date) AS mth, \n",
    "  COUNT(DISTINCT curr_month.user_id) AS monthly_active_users \n",
    "FROM user_actions AS curr_month\n",
    "WHERE EXISTS (\n",
    "  SELECT last_month.user_id \n",
    "  FROM user_actions AS last_month\n",
    "  WHERE last_month.user_id = curr_month.user_id\n",
    "    AND EXTRACT(MONTH FROM last_month.event_date) =\n",
    "    EXTRACT(MONTH FROM curr_month.event_date - interval '1 month')\n",
    ")\n",
    "  AND EXTRACT(MONTH FROM curr_month.event_date) = 7\n",
    "  AND EXTRACT(YEAR FROM curr_month.event_date) = 2022\n",
    "GROUP BY EXTRACT(MONTH FROM curr_month.event_date);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ab57418-6224-418c-a92a-b5ab029cea74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Supercloud Customer\n",
    "Microsoft SQL Interview Question<br>\n",
    "Difficulty: Medium<br>\n",
    "Source: https://datalemur.com/questions/supercloud-customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c57a9ee-1657-4bbf-a789-a95d17960864",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# A Microsoft Azure Supercloud customer is defined as a customer who has purchased at least one product from every product category listed in the products table.\n",
    "\n",
    "# Write a query that identifies the customer IDs of these Supercloud customers.\n",
    "\n",
    "# customer_contracts Table:\n",
    "# Column Name\tType\n",
    "# customer_id\tinteger\n",
    "# product_id\tinteger\n",
    "# amount\tinteger\n",
    "# customer_contracts Example Input:\n",
    "# customer_id\tproduct_id\tamount\n",
    "# 1\t1\t1000\n",
    "# 1\t3\t2000\n",
    "# 1\t5\t1500\n",
    "# 2\t2\t3000\n",
    "# 2\t6\t2000\n",
    "# products Table:\n",
    "# Column Name\tType\n",
    "# product_id\tinteger\n",
    "# product_category\tstring\n",
    "# product_name\tstring\n",
    "# products Example Input:\n",
    "# product_id\tproduct_category\tproduct_name\n",
    "# 1\tAnalytics\tAzure Databricks\n",
    "# 2\tAnalytics\tAzure Stream Analytics\n",
    "# 4\tContainers\tAzure Kubernetes Service\n",
    "# 5\tContainers\tAzure Service Fabric\n",
    "# 6\tCompute\tVirtual Machines\n",
    "# 7\tCompute\tAzure Functions\n",
    "# Example Output:\n",
    "# customer_id\n",
    "# 1\n",
    "# Explanation:\n",
    "# Customer 1 bought from Analytics, Containers, and Compute categories of Azure, and thus is a Supercloud customer. Customer 2 isn't a Supercloud customer, since they don't buy any container services from Azure.\n",
    "\n",
    "# The dataset you are querying against may have different input & output - this is just an example!\n",
    "\n",
    "# Solution1\n",
    "with product_categories as \n",
    "(\n",
    "Select customer_id, count(distinct product_category) as prod_cat_cnt,\n",
    "(Select count(distinct product_category) from products) as tot_prod_cat_cnt\n",
    "from customer_contracts cc \n",
    "join products p\n",
    "on cc.product_id = p.product_id\n",
    "group by customer_id\n",
    ")\n",
    "Select customer_id from product_categories\n",
    "where prod_cat_cnt = tot_prod_cat_cnt\n",
    "\n",
    "# Solution2\n",
    "WITH supercloud_cust AS (\n",
    "  SELECT \n",
    "    customers.customer_id, \n",
    "    COUNT(DISTINCT products.product_category) AS product_count\n",
    "  FROM customer_contracts AS customers\n",
    "  INNER JOIN products \n",
    "    ON customers.product_id = products.product_id\n",
    "  GROUP BY customers.customer_id\n",
    ")\n",
    "\n",
    "SELECT customer_id\n",
    "FROM supercloud_cust\n",
    "WHERE product_count = (\n",
    "  SELECT COUNT(DISTINCT product_category) FROM products\n",
    ");\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb78f0de-465d-4942-96cb-535646e330a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Highest-Grossing Items\n",
    "Amazon SQL Interview Question<br>\n",
    "Difficulty: Medium<br>\n",
    "Source: https://datalemur.com/questions/sql-highest-grossing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a6c104d-0254-40ba-84cd-70b3f24370f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "-- Assume you're given a table containing data on Amazon customers and their spending on products in different category, write a query to identify the top two highest-grossing products within each category in the year 2022. The output should include the category, product, and total spend.\n",
    "\n",
    "-- product_spend Table:\n",
    "-- Column Name\tType\n",
    "-- category\tstring\n",
    "-- product\tstring\n",
    "-- user_id\tinteger\n",
    "-- spend\tdecimal\n",
    "-- transaction_date\ttimestamp\n",
    "-- product_spend Example Input:\n",
    "-- category\tproduct\tuser_id\tspend\ttransaction_date\n",
    "-- appliance\trefrigerator\t165\t246.00\t12/26/2021 12:00:00\n",
    "-- appliance\trefrigerator\t123\t299.99\t03/02/2022 12:00:00\n",
    "-- appliance\twashing machine\t123\t219.80\t03/02/2022 12:00:00\n",
    "-- electronics\tvacuum\t178\t152.00\t04/05/2022 12:00:00\n",
    "-- electronics\twireless headset\t156\t249.90\t07/08/2022 12:00:00\n",
    "-- electronics\tvacuum\t145\t189.00\t07/15/2022 12:00:00\n",
    "-- Example Output:\n",
    "-- category\tproduct\ttotal_spend\n",
    "-- appliance\trefrigerator\t299.99\n",
    "-- appliance\twashing machine\t219.80\n",
    "-- electronics\tvacuum\t341.00\n",
    "-- electronics\twireless headset\t249.90\n",
    "-- Explanation:\n",
    "-- Within the \"appliance\" category, the top two highest-grossing products are \"refrigerator\" and \"washing machine.\"\n",
    "\n",
    "-- In the \"electronics\" category, the top two highest-grossing products are \"vacuum\" and \"wireless headset.\"\n",
    "\n",
    "-- The dataset you are querying against may have different input & output - this is just an example!\n",
    "\n",
    "-- Solution1\n",
    "with category_spend_agg AS\n",
    "(\n",
    "SELECT \n",
    "category, \n",
    "product,\n",
    "year(transaction_date) as  year,\n",
    "sum(spend) as total_spend,\n",
    "row_number() over (partition by category, year(transaction_date)  order by sum(spend) desc) as rnk\n",
    "FROM product_spend\n",
    "group by category, product, year(transaction_date)\n",
    ")\n",
    "Select \n",
    "category,\n",
    "product,\n",
    "total_spend\n",
    "from category_spend_agg\n",
    "where rnk<=2 and year = 2022\n",
    ";\n",
    "\n",
    "-- Solution2\n",
    "WITH ranked_spending_cte AS (\n",
    "  SELECT \n",
    "    category, \n",
    "    product, \n",
    "    SUM(spend) AS total_spend,\n",
    "    RANK() OVER (\n",
    "      PARTITION BY category \n",
    "      ORDER BY SUM(spend) DESC) AS ranking \n",
    "  FROM product_spend\n",
    "  WHERE EXTRACT(YEAR FROM transaction_date) = 2022\n",
    "  GROUP BY category, product\n",
    ")\n",
    "\n",
    "SELECT \n",
    "  category, \n",
    "  product, \n",
    "  total_spend \n",
    "FROM ranked_spending_cte \n",
    "WHERE ranking <= 2 \n",
    "ORDER BY category, ranking;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f17a756e-b386-4b81-85ce-731d0505d2f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Sending vs. Opening Snaps\n",
    "Snapchat SQL Interview Question<br>\n",
    "Difficulty: Medium<br>\n",
    "Source: https://datalemur.com/questions/time-spent-snaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73aa3d35-b933-4aad-87f8-932e9a0ba45d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Assume you're given tables with information on Snapchat users, including their ages and time spent sending and opening snaps.\n",
    "\n",
    "# Write a query to obtain a breakdown of the time spent sending vs. opening snaps as a percentage of total time spent on these activities grouped by age group. Round the percentage to 2 decimal places in the output.\n",
    "\n",
    "# Notes:\n",
    "\n",
    "# Calculate the following percentages:\n",
    "# time spent sending / (Time spent sending + Time spent opening)\n",
    "# Time spent opening / (Time spent sending + Time spent opening)\n",
    "# To avoid integer division in percentages, multiply by 100.0 and not 100.\n",
    "# Effective April 15th, 2023, the solution has been updated and optimised.\n",
    "\n",
    "# activities Table\n",
    "# Column Name\tType\n",
    "# activity_id\tinteger\n",
    "# user_id\tinteger\n",
    "# activity_type\tstring ('send', 'open', 'chat')\n",
    "# time_spent\tfloat\n",
    "# activity_date\tdatetime\n",
    "# activities Example Input\n",
    "# activity_id\tuser_id\tactivity_type\ttime_spent\tactivity_date\n",
    "# 7274\t123\topen\t4.50\t06/22/2022 12:00:00\n",
    "# 2425\t123\tsend\t3.50\t06/22/2022 12:00:00\n",
    "# 1413\t456\tsend\t5.67\t06/23/2022 12:00:00\n",
    "# 1414\t789\tchat\t11.00\t06/25/2022 12:00:00\n",
    "# 2536\t456\topen\t3.00\t06/25/2022 12:00:00\n",
    "# age_breakdown Table\n",
    "# Column Name\tType\n",
    "# user_id\tinteger\n",
    "# age_bucket\tstring ('21-25', '26-30', '31-25')\n",
    "# age_breakdown Example Input\n",
    "# user_id\tage_bucket\n",
    "# 123\t31-35\n",
    "# 456\t26-30\n",
    "# 789\t21-25\n",
    "# Example Output\n",
    "# age_bucket\tsend_perc\topen_perc\n",
    "# 26-30\t65.40\t34.60\n",
    "# 31-35\t43.75\t56.25\n",
    "# Explanation\n",
    "# Using the age bucket 26-30 as example, the time spent sending snaps was 5.67 and the time spent opening snaps was 3.\n",
    "\n",
    "# To calculate the percentage of time spent sending snaps, we divide the time spent sending snaps by the total time spent on sending and opening snaps, which is 5.67 + 3 = 8.67.\n",
    "\n",
    "# So, the percentage of time spent sending snaps is 5.67 / (5.67 + 3) = 65.4%, and the percentage of time spent opening snaps is 3 / (5.67 + 3) = 34.6%.\n",
    "\n",
    "# The dataset you are querying against may have different input & output - this is just an example!\n",
    "\n",
    "# Solution1\n",
    "with user_group_times AS\n",
    "(\n",
    "SELECT distinct age_bucket,\n",
    "sum(time_spent) over(partition by age_bucket) as total_time,\n",
    "sum(case when activity_type = 'send' then time_spent else 0 end) over(partition by age_bucket) as send_time,\n",
    "sum(case when activity_type = 'open' then time_spent else 0 end) over(partition by age_bucket) as open_time\n",
    "FROM activities a\n",
    "join age_breakdown ab\n",
    "on a.user_id = ab.user_id\n",
    "where activity_type in ('send', 'open')\n",
    ")\n",
    "\n",
    "Select age_bucket,\n",
    "round((send_time/total_time)*100.0,2) as send_prc,\n",
    "round((open_time/total_time)*100.0,2) as open_prc\n",
    "from user_group_times\n",
    "order by age_bucket desc\n",
    "\n",
    "#Solution2\n",
    "with user_group_times AS\n",
    "(\n",
    "SELECT distinct age_bucket,\n",
    "sum(time_spent) as total_time,\n",
    "sum(case when activity_type = 'send' then time_spent else 0 end) as send_time,\n",
    "sum(case when activity_type = 'open' then time_spent else 0 end) as open_time\n",
    "FROM activities a\n",
    "join age_breakdown ab\n",
    "on a.user_id = ab.user_id\n",
    "where activity_type in ('send', 'open')\n",
    "group by age_bucket\n",
    ")\n",
    "\n",
    "Select age_bucket,\n",
    "round((send_time/total_time)*100.0,2) as send_prc,\n",
    "round((open_time/total_time)*100.0,2) as open_prc\n",
    "from user_group_times\n",
    "order by age_bucket desc\n",
    "\n",
    "\n",
    "#Solution3\n",
    "  SELECT \n",
    "    age.age_bucket, \n",
    "    SUM(CASE WHEN activities.activity_type = 'send' \n",
    "      THEN activities.time_spent ELSE 0 END) AS send_timespent, \n",
    "    SUM(CASE WHEN activities.activity_type = 'open' \n",
    "      THEN activities.time_spent ELSE 0 END) AS open_timespent, \n",
    "    SUM(activities.time_spent) AS total_timespent \n",
    "  FROM activities\n",
    "  INNER JOIN age_breakdown AS age \n",
    "    ON activities.user_id = age.user_id \n",
    "  WHERE activities.activity_type IN ('send', 'open') \n",
    "  GROUP BY age.age_bucket\n",
    ") \n",
    "\n",
    "SELECT \n",
    "  age_bucket, \n",
    "  ROUND(100.0 * send_timespent / total_timespent, 2) AS send_perc, \n",
    "  ROUND(100.0 * open_timespent / total_timespent, 2) AS open_perc \n",
    "FROM snaps_statistics;\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71adf80f-e793-4acc-93d4-749962d69496",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Tweets' Rolling Averages\n",
    "Twitter SQL Interview Question<br>\n",
    "Difficulty: Medium<br>\n",
    "Source: https://datalemur.com/questions/rolling-average-tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b2255e8-b543-471f-b066-9a115a4fb13a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "-- Given a table of tweet data over a specified time period, calculate the 3-day rolling average of tweets for each user. Output the user ID, tweet date, and rolling averages rounded to 2 decimal places.\n",
    "\n",
    "-- Notes:\n",
    "\n",
    "-- A rolling average, also known as a moving average or running mean is a time-series technique that examines trends in data over a specified period of time.\n",
    "-- In this case, we want to determine how the tweet count for each user changes over a 3-day period.\n",
    "-- Effective April 7th, 2023, the problem statement, solution and hints for this question have been revised.\n",
    "\n",
    "-- tweets Table:\n",
    "-- Column Name\tType\n",
    "-- user_id\tinteger\n",
    "-- tweet_date\ttimestamp\n",
    "-- tweet_count\tinteger\n",
    "-- tweets Example Input:\n",
    "-- user_id\ttweet_date\ttweet_count\n",
    "-- 111\t06/01/2022 00:00:00\t2\n",
    "-- 111\t06/02/2022 00:00:00\t1\n",
    "-- 111\t06/03/2022 00:00:00\t3\n",
    "-- 111\t06/04/2022 00:00:00\t4\n",
    "-- 111\t06/05/2022 00:00:00\t5\n",
    "-- Example Output:\n",
    "-- user_id\ttweet_date\trolling_avg_3d\n",
    "-- 111\t06/01/2022 00:00:00\t2.00\n",
    "-- 111\t06/02/2022 00:00:00\t1.50\n",
    "-- 111\t06/03/2022 00:00:00\t2.00\n",
    "-- 111\t06/04/2022 00:00:00\t2.67\n",
    "-- 111\t06/05/2022 00:00:00\t4.00\n",
    "-- The dataset you are querying against may have different input & output - this is just an example!\n",
    "\n",
    "-- Solution\n",
    "SELECT user_id,\n",
    "tweet_date,\n",
    "Round(avg(tweet_count) over(partition by user_id order by tweet_date rows between 2 preceding and current row), 2) as rolling_avg_3d\n",
    "FROM tweets;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dafd8769-e4bf-47b1-a3f4-16967ed73028",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Top Three Salaries\n",
    "FAANG SQL Interview Question<br>\n",
    "Difficulty: Medium<br>\n",
    "Source: https://datalemur.com/questions/sql-top-three-salaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48b7a49f-4548-458e-8c50-50033aeed158",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- As part of an ongoing analysis of salary distribution within the company, your manager has requested a report identifying high earners in each department. A 'high earner' within a department is defined as an employee with a salary ranking among the top three salaries within that department.\n",
    "\n",
    "-- You're tasked with identifying these high earners across all departments. Write a query to display the employee's name along with their department name and salary. In case of duplicates, sort the results of department name in ascending order, then by salary in descending order. If multiple employees have the same salary, then order them alphabetically.\n",
    "\n",
    "-- Note: Ensure to utilize the appropriate ranking window function to handle duplicate salaries effectively.\n",
    "\n",
    "-- As of June 18th, we have removed the requirement for unique salaries and revised the sorting order for the results.\n",
    "\n",
    "-- employee Schema:\n",
    "-- column_name\ttype\tdescription\n",
    "-- employee_id\tinteger\tThe unique ID of the employee.\n",
    "-- name\tstring\tThe name of the employee.\n",
    "-- salary\tinteger\tThe salary of the employee.\n",
    "-- department_id\tinteger\tThe department ID of the employee.\n",
    "-- manager_id\tinteger\tThe manager ID of the employee.\n",
    "-- employee Example Input:\n",
    "-- employee_id\tname\tsalary\tdepartment_id\tmanager_id\n",
    "-- 1\tEmma Thompson\t3800\t1\t6\n",
    "-- 2\tDaniel Rodriguez\t2230\t1\t7\n",
    "-- 3\tOlivia Smith\t2000\t1\t8\n",
    "-- 4\tNoah Johnson\t6800\t2\t9\n",
    "-- 5\tSophia Martinez\t1750\t1\t11\n",
    "-- 6\tLiam Brown\t13000\t3\t\n",
    "-- 7\tAva Garcia\t12500\t3\t\n",
    "-- 8\tWilliam Davis\t6800\t2\t\n",
    "-- 9\tIsabella Wilson\t11000\t3\t\n",
    "-- 10\tJames Anderson\t4000\t1\t11\n",
    "-- department Schema:\n",
    "-- column_name\ttype\tdescription\n",
    "-- department_id\tinteger\tThe department ID of the employee.\n",
    "-- department_name\tstring\tThe name of the department.\n",
    "-- department Example Input:\n",
    "-- department_id\tdepartment_name\n",
    "-- 1\tData Analytics\n",
    "-- 2\tData Science\n",
    "-- Example Output:\n",
    "-- department_name\tname\tsalary\n",
    "-- Data Analytics\tJames Anderson\t4000\n",
    "-- Data Analytics\tEmma Thompson\t3800\n",
    "-- Data Analytics\tDaniel Rodriguez\t2230\n",
    "-- Data Science\tNoah Johnson\t6800\n",
    "-- Data Science\tWilliam Davis\t6800\n",
    "-- The output displays the high earners in each department.\n",
    "\n",
    "-- In the Data Analytics deaprtment, James Anderson leads with a salary of $4,000, followed by Emma Thompson earning $3,800, and Daniel Rodriguez with $2,230.\n",
    "-- In the Data Science department, both Noah Johnson and William Davis earn $6,800, with Noah listed before William due to alphabetical ordering.\n",
    "\n",
    "-- Solution\n",
    "with dept_wise_salary as\n",
    "(\n",
    "SELECT e.name, \n",
    "d.department_name, \n",
    "e.salary,  \n",
    "dense_rank() over (partition by d.department_name order by salary desc) as rnk\n",
    "FROM employee e\n",
    "join department d\n",
    "on e.department_id = d.department_id\n",
    ")\n",
    "Select name, department_name, salary\n",
    "from dept_wise_salary\n",
    "where rnk<=3\n",
    "order by department_name, salary desc, name;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56f3e96c-9308-4d31-b491-da030843854a",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"name\":178},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1757951424937}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "employee_records = [\n",
    "(1,'Emma Thompson',3800,1,6),\n",
    "(2,'Daniel Rodriguez',2230,1,7),\n",
    "(3,'Olivia Smith',2000,1,8),\n",
    "(4,'Noah Johnson',6800,2,9),\n",
    "(5,'Sophia Martinez',1750,1,11),\n",
    "(6,'Liam Brown',13000,3,None),\n",
    "(7,'Ava Garcia',12500,3,None),\n",
    "(8,'William Davis',6800,2,None),\n",
    "(9,'Isabella Wilson',11000,3,None),\n",
    "(10,'James Anderson',4000,1,11)\n",
    "]\n",
    "department_records = [(1, 'Data Analytics'),\n",
    "(2, 'Data Science')]\n",
    "\n",
    "employee_df = spark.createDataFrame(employee_records,['id','name','salary','department_id','manager_id'])\n",
    "department_df = spark.createDataFrame(department_records,['id','department'])\n",
    "display(employee_df)\n",
    "display(department_df)\n",
    "\n",
    "spec_window = Window.partitionBy(\"department\").orderBy(f.col(\"salary\").desc())\n",
    "highest_earner_df = employee_df.join(department_df, employee_df.department_id == department_df.id, 'inner')\\\n",
    "                                .select(\"name\", \"department\", \"salary\")\\\n",
    "                                    .withColumn(\"rnk\", f.dense_rank().over(spec_window))\\\n",
    "                                        .where(\"rnk<=3\")\\\n",
    "                                            .select(\"name\",\"department\",\"salary\")\\\n",
    "                                                .sort(f.col(\"department\"),f.col(\"salary\").desc(),f.col(\"name\"))\n",
    "display(highest_earner_df)                                            \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09feca69-cca7-40d9-9147-91e8a334f235",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Signup Activation Rate\n",
    "TikTok SQL Interview Question<br>\n",
    "Difficulty: Medium<br>\n",
    "Source: https://datalemur.com/questions/signup-confirmation-rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e45cbc54-f5f9-440b-a942-bbed42f02dc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "-- New TikTok users sign up with their emails. They confirmed their signup by replying to the text confirmation to activate their accounts. Users may receive multiple text messages for account confirmation until they have confirmed their new account.\n",
    "\n",
    "-- A senior analyst is interested to know the activation rate of specified users in the emails table. Write a query to find the activation rate. Round the percentage to 2 decimal places.\n",
    "\n",
    "-- Definitions:\n",
    "\n",
    "-- emails table contain the information of user signup details.\n",
    "-- texts table contains the users' activation information.\n",
    "-- Assumptions:\n",
    "\n",
    "-- The analyst is interested in the activation rate of specific users in the emails table, which may not include all users that could potentially be found in the texts table.\n",
    "-- For example, user 123 in the emails table may not be in the texts table and vice versa.\n",
    "-- Effective April 4th 2023, we added an assumption to the question to provide additional clarity.\n",
    "\n",
    "-- emails Table:\n",
    "-- Column Name\tType\n",
    "-- email_id\tinteger\n",
    "-- user_id\tinteger\n",
    "-- signup_date\tdatetime\n",
    "-- emails Example Input:\n",
    "-- email_id\tuser_id\tsignup_date\n",
    "-- 125\t7771\t06/14/2022 00:00:00\n",
    "-- 236\t6950\t07/01/2022 00:00:00\n",
    "-- 433\t1052\t07/09/2022 00:00:00\n",
    "-- texts Table:\n",
    "-- Column Name\tType\n",
    "-- text_id\tinteger\n",
    "-- email_id\tinteger\n",
    "-- signup_action\tvarchar\n",
    "-- texts Example Input:\n",
    "-- text_id\temail_id\tsignup_action\n",
    "-- 6878\t125\tConfirmed\n",
    "-- 6920\t236\tNot Confirmed\n",
    "-- 6994\t236\tConfirmed\n",
    "-- 'Confirmed' in signup_action means the user has activated their account and successfully completed the signup process.\n",
    "\n",
    "-- Example Output:\n",
    "-- confirm_rate\n",
    "-- 0.67\n",
    "-- Explanation:\n",
    "-- 67% of users have successfully completed their signup and activated their accounts. The remaining 33% have not yet replied to the text to confirm their signup.\n",
    "\n",
    "-- Solution\n",
    "with ranked_texts as\n",
    "(\n",
    "Select *, \n",
    "row_number() over(partition by email_id order by text_id desc) as rnk\n",
    "from texts\n",
    "),\n",
    "user_actions as\n",
    "(\n",
    "SELECT e.email_id, \n",
    "t.signup_action\n",
    "FROM emails e\n",
    "left join ranked_texts t\n",
    "on e.email_id = t.email_id\n",
    "where t.rnk = 1 or t.rnk is null\n",
    ")\n",
    "\n",
    "Select Round(Sum(Case when signup_action = 'Confirmed' then 1.0 else 0.0 End)/count(email_id), 2) as confirm_rate\n",
    "from user_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ffbce31e-568d-40e1-9db9-5838e8343e43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Spotify Streaming History\n",
    "Spotify SQL Interview Question<br>\n",
    "Difficulty: Medium<br>\n",
    "Source: https://datalemur.com/questions/spotify-streaming-history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "970d7646-5307-403d-8358-b44c34935498",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "-- You're given two tables containing data on Spotify users' streaming activity: songs_history which has historical streaming data, and songs_weekly which has data from the current week.\n",
    "\n",
    "-- Write a query that outputs the user ID, song ID, and cumulative count of song plays up to August 4th, 2022, sorted in descending order.\n",
    "\n",
    "-- Assume that there may be new users or songs in the songs_weekly table that are not present in the songs_history table.\n",
    "\n",
    "-- Definitions:\n",
    "\n",
    "-- song_weeklytable only contains data for the week of August 1st to August 7th, 2022.\n",
    "-- songs_history table contains data up to July 31st, 2022. The query should include historical data from this table.\n",
    "-- songs_history Table:\n",
    "-- Column Name\tType\n",
    "-- history_id\tinteger\n",
    "-- user_id\tinteger\n",
    "-- song_id\tinteger\n",
    "-- song_plays\tinteger\n",
    "-- songs_history Example Input:\n",
    "-- history_id\tuser_id\tsong_id\tsong_plays\n",
    "-- 10011\t777\t1238\t11\n",
    "-- 12452\t695\t4520\t1\n",
    "-- song_plays field contains the historical data of the number of times a user has played a particular song.\n",
    "\n",
    "-- songs_weekly Table:\n",
    "-- Column Name\tType\n",
    "-- user_id\tinteger\n",
    "-- song_id\tinteger\n",
    "-- listen_time\tdatetime\n",
    "-- songs_weekly Example Input:\n",
    "-- user_id\tsong_id\tlisten_time\n",
    "-- 777\t1238\t08/01/2022 12:00:00\n",
    "-- 695\t4520\t08/04/2022 08:00:00\n",
    "-- 125\t9630\t08/04/2022 16:00:00\n",
    "-- 695\t9852\t08/07/2022 12:00:00\n",
    "-- Example Output:\n",
    "-- user_id\tsong_id\tsong_plays\n",
    "-- 777\t1238\t12\n",
    "-- 695\t4520\t2\n",
    "-- 125\t9630\t1\n",
    "-- On 4 August 2022, the data shows that User 777 listened to the song with song ID 1238 for a total of 12 times, with 11 of those times occurring before the current week and 1 time occurring within the current week.\n",
    "\n",
    "-- However, the streaming data for User 695 with the song ID 9852 are not included in the output because the streaming date for that record falls outside the date range specified in the question.\n",
    "\n",
    "-- Solution\n",
    "\n",
    "with songs_weekly_agg AS\n",
    "(\n",
    "Select user_id, song_id, count(*) as weekly_song_plays\n",
    "from songs_weekly\n",
    "where listen_time < '2022-08-05'\n",
    "group by user_id, song_id\n",
    ")\n",
    "SELECT  \n",
    "coalesce(sh.user_id,sw.user_id) as user_id, \n",
    "coalesce(sh.song_id,sw.song_id) as song_id, \n",
    "(coalesce(sh.song_plays,0) + coalesce(sw.weekly_song_plays,0)) as song_plays \n",
    "FROM songs_history sh\n",
    "full join songs_weekly_agg sw\n",
    "on sh.user_id = sw.user_id\n",
    "and sh.song_id = sw.song_id\n",
    "order by (coalesce(sh.song_plays,0) + coalesce(sw.weekly_song_plays,0)) desc;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d91a2cb4-c171-4881-9eaa-036fbc9da91a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Odd and Even Measurements\n",
    "Google SQL Interview Question<br>\n",
    "Difficulty: Medium<br>\n",
    "Source: https://datalemur.com/questions/odd-even-measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70efb8ea-07c6-47cf-aa91-84946eb7d4e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "-- Assume you're given a table with measurement values obtained from a Google sensor over multiple days with measurements taken multiple times within each day.\n",
    "\n",
    "-- Write a query to calculate the sum of odd-numbered and even-numbered measurements separately for a particular day and display the results in two different columns. Refer to the Example Output below for the desired format.\n",
    "\n",
    "-- Definition:\n",
    "\n",
    "-- Within a day, measurements taken at 1st, 3rd, and 5th times are considered odd-numbered measurements, and measurements taken at 2nd, 4th, and 6th times are considered even-numbered measurements.\n",
    "-- Effective April 15th, 2023, the question and solution for this question have been revised.\n",
    "\n",
    "-- measurements Table:\n",
    "-- Column Name\tType\n",
    "-- measurement_id\tinteger\n",
    "-- measurement_value\tdecimal\n",
    "-- measurement_time\tdatetime\n",
    "-- measurements Example Input:\n",
    "-- measurement_id\tmeasurement_value\tmeasurement_time\n",
    "-- 131233\t1109.51\t07/10/2022 09:00:00\n",
    "-- 135211\t1662.74\t07/10/2022 11:00:00\n",
    "-- 523542\t1246.24\t07/10/2022 13:15:00\n",
    "-- 143562\t1124.50\t07/11/2022 15:00:00\n",
    "-- 346462\t1234.14\t07/11/2022 16:45:00\n",
    "-- Example Output:\n",
    "-- measurement_day\todd_sum\teven_sum\n",
    "-- 07/10/2022 00:00:00\t2355.75\t1662.74\n",
    "-- 07/11/2022 00:00:00\t1124.50\t1234.14\n",
    "-- Explanation\n",
    "-- Based on the results,\n",
    "\n",
    "-- On 07/10/2022, the sum of the odd-numbered measurements is 2355.75, while the sum of the even-numbered measurements is 1662.74.\n",
    "-- On 07/11/2022, there are only two measurements available. The sum of the odd-numbered measurements is 1124.50, and the sum of the even-numbered measurements is 1234.14.\n",
    "-- The dataset you are querying against may have different input & output - this is just an example!\n",
    "\n",
    "-- Solution\n",
    "with daywise_ranked_measurments as\n",
    "(\n",
    "SELECT date_trunc('day', measurement_time) as measurement_day, \n",
    "row_number() over (partition by date_trunc('day', measurement_time) order by measurement_time) as rnk\n",
    "FROM measurements\n",
    ")\n",
    "Select measurement_day,\n",
    "sum(case when rnk%2<>0 then measurement_value else 0 end) as odd_sum,\n",
    "sum(case when rnk%2==0 then measurement_value else 0 end) as even_sum\n",
    "from daywise_ranked_measurments\n",
    "group by measurement_day\n",
    "order by measurement_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cda483b-369e-4279-941e-af359f145722",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "measurement_records = [\n",
    "(131233,1109.51,'07/10/2022 09:00:00'),\n",
    "(135211,1662.74,'07/10/2022 11:00:00'),\n",
    "(523542,1246.24,'07/10/2022 13:15:00'),\n",
    "(143562,1124.50,'07/11/2022 15:00:00'),\n",
    "(346462,1234.14,'07/11/2022 16:45:00')\n",
    "]\n",
    "\n",
    "measurement_cols = ['measurement_id', 'measurement_value', 'measurement_time']\n",
    "\n",
    "measurement_df = spark.createDataFrame(measurement_records, measurement_cols)\n",
    "# display(measurement_df)\n",
    "measurement_df = measurement_df.withColumn(\"measurement_time\", to_timestamp(\"measurement_time\", \"MM/dd/yyyy HH:mm:ss\"))\n",
    "display(measurement_df)\n",
    "measurement_df.createOrReplaceTempView(\"measurements\")\n",
    "\n",
    "result_df = spark.sql(\"\"\"with daywise_ranked_measurments as\n",
    "(\n",
    "SELECT date_trunc('day', measurement_time) as measurement_day, \n",
    "measurement_value,\n",
    "row_number() over (partition by date_trunc('day', measurement_time) order by measurement_time) as rnk\n",
    "FROM measurements\n",
    ")\n",
    "Select measurement_day,\n",
    "sum(case when rnk%2<>0 then measurement_value else 0 end) as odd_sum,\n",
    "sum(case when rnk%2==0 then measurement_value else 0 end) as even_sum\n",
    "from daywise_ranked_measurments\n",
    "group by measurement_day\n",
    "order by measurement_day\n",
    "\"\"\")\n",
    "display(result_df)\n",
    "\n",
    "\n",
    "spec_window = Window.partitionBy(\"measurement_day\").orderBy(\"measurement_time\")\n",
    "result_df_2 = measurement_df.withColumn(\"measurement_day\", date_trunc('day', \"measurement_time\"))\\\n",
    "                            .withColumn(\"rnk\", row_number().over(spec_window))\\\n",
    "                                .select(\"measurement_day\",\"measurement_value\", \"rnk\")\\\n",
    "                                    .groupBy(\"measurement_day\")\\\n",
    "                                    .agg(sum(expr(\"case when rnk%2!=0 then measurement_value else 0 end\")).alias(\"odd_sum\"),\n",
    "                                         sum(expr(\"case when rnk%2==0 then measurement_value else 0 end\")).alias(\"even_sum\"))\\\n",
    "                                             .select(\"measurement_day\", \"odd_sum\", \"even_sum\")\n",
    "display(result_df_2)\n",
    "                                             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7f272a7-59fa-445c-aa6a-22ba773c1b7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "-- Zomato is a leading online food delivery service that connects users with various restaurants and cuisines, allowing them to browse menus, place orders, and get meals delivered to their doorsteps.\n",
    "\n",
    "-- Recently, Zomato encountered an issue with their delivery system. Due to an error in the delivery driver instructions, each item's order was swapped with the item in the subsequent row. As a data analyst, you're asked to correct this swapping error and return the proper pairing of order ID and item.\n",
    "\n",
    "-- If the last item has an odd order ID, it should remain as the last item in the corrected data. For example, if the last item is Order ID 7 Tandoori Chicken, then it should remain as Order ID 7 in the corrected data.\n",
    "\n",
    "-- In the results, return the correct pairs of order IDs and items.\n",
    "\n",
    "-- orders Schema:\n",
    "-- column_name\ttype\tdescription\n",
    "-- order_id\tinteger\tThe ID of each Zomato order.\n",
    "-- item\tstring\tThe name of the food item in each order.\n",
    "-- orders Example Input:\n",
    "-- Here's a sample of the initial incorrect data:\n",
    "\n",
    "-- order_id\titem\n",
    "-- 1\tChow Mein\n",
    "-- 2\tPizza\n",
    "-- 3\tPad Thai\n",
    "-- 4\tButter Chicken\n",
    "-- 5\tEggrolls\n",
    "-- 6\tBurger\n",
    "-- 7\tTandoori Chicken\n",
    "-- orders Example Output:\n",
    "-- The corrected data should look like this:\n",
    "\n",
    "-- corrected_order_id\titem\n",
    "-- 1\tPizza\n",
    "-- 2\tChow Mein\n",
    "-- 3\tButter Chicken\n",
    "-- 4\tPad Thai\n",
    "-- 5\tBurger\n",
    "-- 6\tEggrolls\n",
    "-- 7\tTandoori Chicken\n",
    "-- Order ID 1 is now associated with Pizza and Order ID 2 is paired with Chow Mein. This adjustment ensures that each order is correctly aligned with its respective item, addressing the initial swapping error.\n",
    "\n",
    "-- Order ID 7 remains unchanged and is still associated with Tandoori Chicken. This preserves the order sequence ensuring that the last odd order ID remains unaltered.\n",
    "\n",
    "-- Solution\n",
    "\n",
    "with orders_modified as\n",
    "(\n",
    "SELECT order_id as corrected_order_id,\n",
    "item,\n",
    "lag(item) over(order by order_id) as lag_item,\n",
    "lead(item) over(order by order_id) as lead_item,\n",
    "max(order_id) over() as max_order_id\n",
    "FROM orders\n",
    ")\n",
    "Select corrected_order_id,\n",
    "Case \n",
    "When corrected_order_id%2<>0 and corrected_order_id<>max_order_id then lead_item\n",
    "When corrected_order_id%2=0 then lag_item\n",
    "Else item \n",
    "End as item\n",
    "from orders_modified;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00b8d13f-3d8d-4c46-b2bc-72fbf0396201",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql. functions import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "orders_records = [\n",
    "(1, 'Chow Mein'),\n",
    "(2, 'Pizza'),\n",
    "(3, 'Pad Thai'),\n",
    "(4, 'Butter Chicken'),\n",
    "(5, 'Eggrolls'),\n",
    "(6, 'Burger'),\n",
    "(7, 'Tandoori Chicken')\n",
    "]\n",
    "\n",
    "cols = ['order_id', 'item']\n",
    "orders_df = spark.createDataFrame(orders_records, cols)\n",
    "display(orders_df)\n",
    "\n",
    "spec_window = Window.orderBy(\"order_id\")\n",
    "spec_window2 = Window.rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "result_df1 = orders_df.withColumn(\"lag_item\", lag(\"item\").over(spec_window))\\\n",
    "                        .withColumn(\"lead_item\", lead(\"item\").over(spec_window))\\\n",
    "                            .withColumn(\"max_order_id\", max(\"order_id\").over(spec_window2))\n",
    "display(result_df1)\n",
    "result_df2 = result_df1.withColumn(\"item\", when((col(\"order_id\")%2!=0) & \n",
    "                                                    (col(\"max_order_id\")!=col(\"order_id\")), \n",
    "                                                    col(\"lead_item\"))\n",
    "                                            .when(col(\"order_id\")%2==0, col(\"lag_item\"))\n",
    "                                            .otherwise(col(\"item\")))\\\n",
    "                                    .select(expr(\"order_id as Corrected_Order_Id\"), \"item\")\n",
    "display(result_df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "031d540d-237d-42af-b562-84674d54c7e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##FAANG Stock Min-Max (Part 1)\n",
    "Bloomberg SQL Interview Question<br>\n",
    "Difficulty: Medium<br>\n",
    "Source: https://datalemur.com/questions/sql-bloomberg-stock-min-max-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "477e1b14-cc0a-4273-8817-1f4dbd5e8c88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "-- The Bloomberg terminal is the go-to resource for financial professionals, offering convenient access to a wide array of financial datasets. As a Data Analyst at Bloomberg, you have access to historical data on stock performance.\n",
    "\n",
    "-- Currently, you're analyzing the highest and lowest open prices for each FAANG stock by month over the years.\n",
    "\n",
    "-- For each FAANG stock, display the ticker symbol, the month and year ('Mon-YYYY') with the corresponding highest and lowest open prices (refer to the Example Output format). Ensure that the results are sorted by ticker symbol.\n",
    "\n",
    "-- stock_prices Schema:\n",
    "-- Column Name\tType\tDescription\n",
    "-- date\tdatetime\tThe specified date (mm/dd/yyyy) of the stock data.\n",
    "-- ticker\tvarchar\tThe stock ticker symbol (e.g., AAPL) for the corresponding company.\n",
    "-- open\tdecimal\tThe opening price of the stock at the start of the trading day.\n",
    "-- high\tdecimal\tThe highest price reached by the stock during the trading day.\n",
    "-- low\tdecimal\tThe lowest price reached by the stock during the trading day.\n",
    "-- close\tdecimal\tThe closing price of the stock at the end of the trading day.\n",
    "-- stock_prices Example Input:\n",
    "-- Note that the table below displays randomly selected AAPL data.\n",
    "\n",
    "-- date\tticker\topen\thigh\tlow\tclose\n",
    "-- 01/31/2023 00:00:00\tAAPL\t142.28\t142.70\t144.34\t144.29\n",
    "-- 02/28/2023 00:00:00\tAAPL\t146.83\t147.05\t149.08\t147.41\n",
    "-- 03/31/2023 00:00:00\tAAPL\t161.91\t162.44\t165.00\t164.90\n",
    "-- 04/30/2023 00:00:00\tAAPL\t167.88\t168.49\t169.85\t169.68\n",
    "-- 05/31/2023 00:00:00\tAAPL\t176.76\t177.33\t179.35\t177.25\n",
    "-- Example Output:\n",
    "-- ticker\thighest_mth\thighest_open\tlowest_mth\tlowest_open\n",
    "-- AAPL\tMay-2023\t176.76\tJan-2023\t142.28\n",
    "-- Apple Inc. (AAPL) achieved its highest opening price of $176.76 in May 2023 and its lowest opening price of $142.28 in January 2023.\n",
    "\n",
    "-- Solution\n",
    "with stock_prices_partitioned\n",
    "as (\n",
    "SELECT *,\n",
    "max(open) over(partition by ticker) as max_open,\n",
    "min(open) over(partition by ticker) as min_open\n",
    "FROM stock_prices\n",
    "),\n",
    "max_stock_prices\n",
    "as (\n",
    "Select ticker, \n",
    "concat(to_char(date, 'Mon'), '-', cast(date_part('year',date) as varchar)) as highest_mth,\n",
    "open as highest_open\n",
    "from stock_prices_partitioned\n",
    "where open = max_open \n",
    "),\n",
    "min_stock_prices\n",
    "as (\n",
    "Select ticker, \n",
    "concat(to_char(date, 'Mon'), '-', cast(date_part('year',date) as varchar)) as lowest_mth,\n",
    "open as lowest_open\n",
    "from stock_prices_partitioned\n",
    "where open = min_open \n",
    ")\n",
    "Select h.ticker,\n",
    "h.highest_mth,\n",
    "h.highest_open,\n",
    "l.lowest_mth,\n",
    "l.lowest_open\n",
    "from max_stock_prices h\n",
    "join min_stock_prices l\n",
    "on h.ticker = l.ticker\n",
    "order by ticker\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f47040d4-c85d-4e7f-b35b-2eb305d090bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##User Shopping Sprees\n",
    "Amazon SQL Interview Question<br>\n",
    "Difficulty: Medium<br>\n",
    "Source: https://datalemur.com/questions/amazon-shopping-spree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1107f0c6-4a9e-4f41-985b-ae314d3dedd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "-- In an effort to identify high-value customers, Amazon asked for your help to obtain data about users who go on shopping sprees. A shopping spree occurs when a user makes purchases on 3 or more consecutive days.\n",
    "\n",
    "-- List the user IDs who have gone on at least 1 shopping spree in ascending order.\n",
    "\n",
    "-- transactions Table:\n",
    "-- Column Name\tType\n",
    "-- user_id\tinteger\n",
    "-- amount\tfloat\n",
    "-- transaction_date\ttimestamp\n",
    "-- transactions Example Input:\n",
    "-- user_id\tamount\ttransaction_date\n",
    "-- 1\t9.99\t08/01/2022 10:00:00\n",
    "-- 1\t55\t08/17/2022 10:00:00\n",
    "-- 2\t149.5\t08/05/2022 10:00:00\n",
    "-- 2\t4.89\t08/06/2022 10:00:00\n",
    "-- 2\t34\t08/07/2022 10:00:00\n",
    "-- Example Output:\n",
    "-- user_id\n",
    "-- 2\n",
    "-- Explanation\n",
    "-- In this example, user_id 2 is the only one who has gone on a shopping spree.\n",
    "\n",
    "-- Solution\n",
    "with transactions_diff as\n",
    "(SELECT user_id, \n",
    "transaction_date,\n",
    "lead(transaction_date) over(partition by user_id order by transaction_date) as next_transaction_date,\n",
    "--datediff('day', lead(transaction_date) over(partition by user_id order by transaction_date), transaction_date) as days_diff\n",
    "(lead(transaction_date) over(partition by user_id order by transaction_date)::date - transaction_date::date) as days_diff\n",
    "FROM transactions\n",
    "),\n",
    "consecutive_transactions as\n",
    "(Select user_id,\n",
    "(lead(days_diff) over(partition by user_id order by transaction_date) - days_diff) as diff\n",
    "from transactions_diff\n",
    ")\n",
    "Select distinct user_id \n",
    "from consecutive_transactions\n",
    "where diff = 0;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1eaa06fb-a2fb-48e1-af34-39eb14009bed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Histogram of Users and Purchases\n",
    "Walmart SQL Interview Question<br>\n",
    "Difficulty: Medium<br>\n",
    "Source: https://datalemur.com/questions/histogram-users-purchases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb3a0819-48fa-466f-bed0-01ff5bf200f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "-- Assume you're given a table on Walmart user transactions. Based on their most recent transaction date, write a query that retrieve the users along with the number of products they bought.\n",
    "\n",
    "-- Output the user's most recent transaction date, user ID, and the number of products, sorted in chronological order by the transaction date.\n",
    "\n",
    "-- user_transactions Table:\n",
    "-- Column Name\tType\n",
    "-- product_id\tinteger\n",
    "-- user_id\tinteger\n",
    "-- spend\tdecimal\n",
    "-- transaction_date\ttimestamp\n",
    "-- user_transactions Example Input:\n",
    "-- product_id\tuser_id\tspend\ttransaction_date\n",
    "-- 3673\t123\t68.90\t07/08/2022 12:00:00\n",
    "-- 9623\t123\t274.10\t07/08/2022 12:00:00\n",
    "-- 1467\t115\t19.90\t07/08/2022 12:00:00\n",
    "-- 2513\t159\t25.00\t07/08/2022 12:00:00\n",
    "-- 1452\t159\t74.50\t07/10/2022 12:00:00\n",
    "-- Example Output:\n",
    "-- transaction_date\tuser_id\tpurchase_count\n",
    "-- 07/08/2022 12:00:00\t115\t1\n",
    "-- 07/08/2022 12:00:000\t123\t2\n",
    "-- 07/10/2022 12:00:00\t159\t1\n",
    "\n",
    "-- Solution\n",
    "with raked_user_transactions as\n",
    "(SELECT user_id, \n",
    "product_id, \n",
    "transaction_date, \n",
    "dense_rank() over(partition by user_id order by transaction_date desc) as rnk\n",
    "FROM user_transactions\n",
    ")\n",
    "Select transaction_date, \n",
    "user_id,  \n",
    "count(product_id) as purchase_count\n",
    "from raked_user_transactions\n",
    "where rnk = 1\n",
    "group by transaction_date, user_id\n",
    "order by transaction_date;\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "SQL Practice",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
